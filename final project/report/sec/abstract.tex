\begin{abstract}

We present a method that uses deformable Gaussian representation based on monocular RGBD input, aimed at accomplishing the task of 
dynamic novel view synthesis. The 2D images with depth are back-projected into 3D point clouds to initialize Gaussians, and the flow between frames is utilized to generate warped Gaussians as the priors of the deformable network. RANS encoding and quantization were also used to do residue compress for improvements. Our method balanced the training, rendering time, and quantities of novel view, achieving monocular dynamic novel view synthesis.
\end{abstract}